7th_assi

pip install nltk


import nltk



nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')



from nltk.tokenize import sent_tokenize
tokenized_text= sent_tokenize(text)
print(tokenized_text)


from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)



import re


text= "How to remove stop words with NLTK library in Python?"
text= re.sub('[^a-zA-Z]', ' ',text)
tokens = word_tokenize(text.lower())
filtered_text=[]
for w in tokens:
 if w not in stop_words:
 filtered_text.append(w)
print("Tokenized Sentence:",tokens)
print("Filterd Sentence:",filtered_text)




from nltk.stem import PorterStemmer
e_words= ["wait", "waiting", "waited", "waits"]
ps =PorterStemmer()
for w in e_words:
 rootWord=ps.stem(w)
print(rootWord)



from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
 print("Lemma for {} is {}".format(w,
wordnet_lemmatizer.lemmatize(w)))



import nltk
from nltk.tokenize import word_tokenize
data="The pink sweater fit her perfectly"
words=word_tokenize(data)
for word in words:
 print(nltk.pos_tag([word]))


import nltk
paragraph = """I have three visions for India. In 3000 years of our history,
 the world have come and invaded us, captured our lands, conquer
 Yet we have not done this to any other nation. We have not conq
 We have not grabbed their land, their culture,
 their history and tried to enforce our way of life on them.
 Why? """



import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer



wn=WordNetLemmatizer()
sentences=nltk.sent_tokenize(paragraph)


corpus=[]
for i in range(len(sentences)):
 review=re.sub('[^a-zA-Z]',' ',sentences[i])
 review=review.lower()
 review=review.split()
 review=[wn.lemmatize(word) for word in review if not word in set(stopwords
 review=' '.join(review)
 corpus.append(review)
corpus



from sklearn.feature_extraction.text import TfidfVectorizer
tf=TfidfVectorizer()
X=tf.fit_transform(corpus).toarray()
print(X)


















































































